{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto ML Classidfication\n",
    "\n",
    "> Automatically trains Models for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-876f830a876a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# load JS visualization code to notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitjs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "#export \n",
    "import streamlit as st\n",
    "import streamlit.components.v1 as components\n",
    "from pdpbox import pdp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shap\n",
    "# load JS visualization code to notebook\n",
    "shap.initjs()\n",
    "\n",
    "import base64\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "\n",
    "#Simple Classifiers\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Tree based Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Gradient Based Classifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Preprocessing packages\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import random\n",
    "\n",
    "import os\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "def model_predict(data_asarray):\n",
    "    data_asframe =  pd.DataFrame(data_asarray, columns=my_cols)\n",
    "    return gscv.predict_proba(data_asframe)\n",
    "\n",
    "\n",
    "def combined_metrics(X_test, y_test, clf):\n",
    "    #to be used in combined metrics function.\n",
    "#     enc = LabelEncoder()\n",
    "#     trans_y_train = enc.fit_transform(y_train)\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "    ohe.fit(y_train.values.reshape(-1, 1))\n",
    "    y_test_ohe = ohe.transform(y_test.values.reshape(-1, 1)).toarray()\n",
    "    \n",
    "    metrics_list = [[accuracy_score(y_test, \n",
    "                                    clf.predict(X_test))], \n",
    "                    [precision_score(y_test, clf.predict(X_test), average = 'micro')],\n",
    "\n",
    "                    [recall_score(y_test, clf.predict(X_test), average = 'micro')], \n",
    "                    \n",
    "                    [f1_score(y_test, clf.predict(X_test), average = 'micro')],\n",
    "                    \n",
    "                    [roc_auc_score(y_test_ohe, ohe.transform(clf.predict(X_test).reshape(-1, 1)).toarray(), multi_class='ovr')],\n",
    "                    \n",
    "                    [hamming_loss(y_test, clf.predict(X_test))], \n",
    "                   \n",
    "                    [log_loss(y_test_ohe, ohe.transform(clf.predict(X_test).reshape(-1, 1)).toarray())]\n",
    "                   ]\n",
    "    \n",
    "\n",
    "    index = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC', 'Hamming Loss', 'Log Loss']\n",
    "#     index = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'Hamming Loss', 'Log Loss']\n",
    "#     index = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC', 'Hamming Loss']\n",
    "\n",
    "    df_metric = pd.DataFrame(metrics_list, index = index, columns = ['Value'])\n",
    "\n",
    "    return df_metric\n",
    "\n",
    "\n",
    "def confusion_matrix_plot(cm, class_names, title = 'Confusion Matrix Plot'):\n",
    "    plt.clf()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap = 'Blues_r')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Predicted')\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    s = [['TN','FP'], ['FN', 'TP']]\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            plt.text(j,i, str(cm[i][j]))\n",
    "    plt.show()\n",
    "            \n",
    "\n",
    "def to_excel(df):\n",
    "    output = BytesIO()\n",
    "    writer = pd.ExcelWriter(output, engine='xlsxwriter')\n",
    "    df.to_excel(writer, index = False, sheet_name='Sheet1')\n",
    "    workbook  = writer.book\n",
    "    worksheet = writer.sheets['Sheet1']\n",
    "    format1 = workbook.add_format({'num_format': '0.00'}) # Tried with '0%' and '#,##0.00' also.\n",
    "    worksheet.set_column('A:A', None, format1) # Say Data are in column A\n",
    "    writer.save()\n",
    "    processed_data = output.getvalue()\n",
    "    return processed_data\n",
    "\n",
    "def get_table_download_link(df):\n",
    "    \"\"\"Generates a link allowing the data in a given panda dataframe to be downloaded\n",
    "    in:  dataframe\n",
    "    out: href string\n",
    "    \"\"\"\n",
    "    val = to_excel(df)\n",
    "    b64 = base64.b64encode(val)  # val looks like b'...'\n",
    "    return f'<a href=\"data:application/octet-stream;base64,{b64.decode()}\" download=\"Your_File.xlsx\">Download output file</a>' # decode b'abc' => abc\n",
    "\n",
    "\n",
    "def GNB():\n",
    "    gnb_params = {'clf__estimator':[GaussianNB()]\n",
    "                 }\n",
    "    return gnb_params\n",
    "    \n",
    "def LogisticReg():\n",
    "    lr_params = {'clf__estimator': [LogisticRegression()]\n",
    "                    }\n",
    "    \n",
    "    st.subheader('Logistic Regression')\n",
    "    penalty = st.multiselect('Penalty', ['l1', 'l2'], ['l2'])\n",
    "    reg = st.multiselect('C', [0.1, 1.0, 2.0], [1.0])\n",
    "    solver = st.multiselect('Solver', ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'], ['liblinear'])\n",
    "    lr_params['clf__estimator__penalty'] = penalty\n",
    "    lr_params['clf__estimator__C'] = reg\n",
    "    lr_params['clf__estimator__solver'] = solver\n",
    "    return lr_params\n",
    "    \n",
    "def KNN():\n",
    "    knn_params = {'clf__estimator': [KNeighborsClassifier()]\n",
    "                 }\n",
    "    \n",
    "    st.subheader('KNN')\n",
    "    n_neighbors = st.multiselect('Neighbors', list(range(1,30)), [5])\n",
    "    leaf_size = st.multiselect('Leaf Size', list(range(1,50)), [30])\n",
    "    p_distance = st.multiselect('Distance Metric', [1,2], [2])\n",
    "    \n",
    "    knn_params['clf__estimator__n_neighbors'] = n_neighbors\n",
    "    knn_params['clf__estimator__leaf_size'] = leaf_size\n",
    "    knn_params['clf__estimator__p'] = p_distance\n",
    "    return knn_params\n",
    "\n",
    "def SVM():\n",
    "    svm_params = {'clf__estimator': [SVC(probability=True)]\n",
    "                 }\n",
    "    \n",
    "    st.subheader('Support Vector Machines')\n",
    "    c = st.multiselect('C', [0.1, 1, 10, 100, 1000], [1])\n",
    "    gamma = st.multiselect('Gamma', ['scale', 'auto'], ['scale'])\n",
    "    kernel = st.multiselect('Kernel', ['linear', 'rbf', 'poly', 'sigmoid'], ['rbf'])\n",
    "    \n",
    "    svm_params['clf__estimator__C'] = c\n",
    "    svm_params['clf__estimator__gamma'] = gamma\n",
    "    svm_params['clf__estimator__kernel'] = kernel\n",
    "    return svm_params\n",
    "    \n",
    "\n",
    "def DT():\n",
    "    dt_params = {'clf__estimator': [DecisionTreeClassifier()]}\n",
    "    \n",
    "    st.subheader('Decision Tree')\n",
    "    criterion = st.multiselect('Criterion', [\"gini\", \"entropy\"], ['gini'])\n",
    "    min_samp_split = st.multiselect('Min Samples Split', [2, 10], [2])\n",
    "    max_depth = st.multiselect('Max Depth', [2, 5, 10], [10])\n",
    "    \n",
    "    dt_params['clf__estimator__criterion'] = criterion\n",
    "    dt_params['clf__estimator__min_samples_leaf'] = min_samp_split\n",
    "    dt_params['clf__estimator__max_depth'] = max_depth\n",
    "    return dt_params\n",
    "    \n",
    "def RF():\n",
    "    rf_params = {'clf__estimator': [RandomForestClassifier()]\n",
    "                }\n",
    "    \n",
    "    st.subheader('Random Forest')\n",
    "    n_estimators = st.multiselect('Number of Trees', [100, 200, 500], [100]) \n",
    "    max_features = st.multiselect('Max Features', [2, 10, 'auto', 'sqrt', 'log2'], ['auto'])\n",
    "    max_depth = st.multiselect('Max Depth', [4,5,6,7,8, None], [None])\n",
    "    criterion = st.multiselect('Criteria', ['gini', 'entropy'], ['gini'])\n",
    "    \n",
    "    rf_params['clf__estimator__n_estimators'] = n_estimators\n",
    "    rf_params['clf__estimator__max_features'] = max_features \n",
    "    rf_params['clf__estimator__max_depth'] = max_depth\n",
    "    rf_params['clf__estimator__criterion'] = criterion\n",
    "    return rf_params\n",
    "\n",
    "def GB():\n",
    "    gb_params = {'clf__estimator': [GradientBoostingClassifier()]\n",
    "            }\n",
    "    \n",
    "    st.subheader('Gradient Booster')\n",
    "    loss = st.multiselect('Loss Function', ['deviance', 'exponential'], ['deviance'])\n",
    "    learning_rate = st.multiselect('Learning Rate', [0.001, 0.01, 0.1], [0.1])\n",
    "    min_samples_split = st.multiselect('Min Samples Split', list(range(1, 10)), [2])\n",
    "    min_samples_leaf = st.multiselect('Min Samples Leaf', list(range(1, 10)), [1])\n",
    "    max_depth = st.multiselect('Max Depth', [1, 2, 3, 4, 5, 6], [3])\n",
    "    max_features = st.multiselect('Max Features', ['auto', 'log2', 'sqrt', None], [None])\n",
    "    criterion = st.multiselect('Criterion', ['friedman_mse', 'mse', 'mae'], ['friedman_mse'])\n",
    "    subsample = st.multiselect('Subsample', [0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0], [1.0])\n",
    "    n_estimators = st.multiselect('Number of Trees', [50, 100, 150, 200, 250], [100])\n",
    "    \n",
    "    gb_params['clf__estimator__loss'] = loss\n",
    "    gb_params['clf__estimator__learning_rate'] = learning_rate\n",
    "    gb_params['clf__estimator__min_samples_split'] = min_samples_split\n",
    "    gb_params['clf__estimator__min_samples_leaf'] = min_samples_leaf\n",
    "    gb_params['clf__estimator__max_depth'] = max_depth\n",
    "    gb_params['clf__estimator__max_features']  = max_features\n",
    "    gb_params['clf__estimator__criterion'] = criterion\n",
    "    gb_params['clf__estimator__subsample'] = subsample\n",
    "    gb_params['clf__estimator__n_estimators'] = n_estimators\n",
    "    return gb_params\n",
    "    \n",
    "    \n",
    "def ERT():\n",
    "    ert_params = {'clf__estimator': [ExtraTreesClassifier()]\n",
    "                 }\n",
    "    \n",
    "    st.subheader('Extra Random Trees')\n",
    "    n_estimators = st.multiselect('Number of Trees', [100, 200, 500, 1000], [100]) #fix\n",
    "    max_depth = st.multiselect('Max Depth', [None, 4, 5, 6, 7, 8, 9], [None])  #fix\n",
    "    min_samples_leaf = st.multiselect('Min Sample per Leaf', [1, 2, 3, 4, 5], [1])\n",
    "    n_jobs = st.selectbox('Parallelism', [1, 2, 3, 4, -1], 4) \n",
    "    \n",
    "    ert_params['clf__estimator__n_estimators'] = n_estimators\n",
    "    ert_params['clf__estimator__max_depth'] = max_depth\n",
    "    ert_params['clf__estimator__min_samples_leaf'] = min_samples_leaf\n",
    "    ert_params['clf__estimator__n_jobs'] = [n_jobs]\n",
    "    return ert_params\n",
    "    \n",
    "def XGB():\n",
    "    xgb_params ={'clf__estimator':[XGBClassifier()]\n",
    "                }\n",
    "    \n",
    "    st.subheader('XGBoost')\n",
    "    n_estimators = st.multiselect('Number of Trees', list(range(50, 1000, 50)), [50]) #fix\n",
    "    max_depth = st.multiselect('Max Depth', list(range(1, 20)), [6])  #fix\n",
    "    min_child_weight = st.multiselect('Min Child Weight', list(range(1, 10, 1)), [1])\n",
    "    gamma = st.multiselect('Gamma', list(range(0, 10)), [1]) \n",
    "    learning_rate = st.multiselect('Learning Rate', [0.01, 0.05, 0.1, 0.2, 0.3], [0.3])\n",
    "    subsample = st.multiselect('Subsample', list(np.divide(range(5, 11), 10)), [1.0])\n",
    "    booster = st.multiselect('Booster', ['gbtree', 'gblinear'], ['gbtree'])\n",
    "    \n",
    "    xgb_params['clf__estimator__n_estimators'] = n_estimators\n",
    "    xgb_params['clf__estimator__max_depth'] = max_depth\n",
    "    xgb_params['clf__estimator__min_child_weight'] =  min_child_weight\n",
    "    xgb_params['clf__estimator__gamma'] = gamma \n",
    "    xgb_params['clf__estimator__learning_rate'] = learning_rate\n",
    "    xgb_params['clf__estimator__subsample'] = subsample\n",
    "    xgb_params['clf__estimator__booster'] = booster\n",
    "    return xgb_params\n",
    "        \n",
    "def SGD():\n",
    "    sgd_params = {'clf__estimator': [SGDClassifier()]\n",
    "                 }\n",
    "    \n",
    "    st.subheader('SGD')\n",
    "    loss = st.multiselect('Loss Function', ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'], ['hinge']) #fix\n",
    "    max_iter = st.multiselect('Max Iterations', list(np.multiply(range(5, 16),  100)), [1000])  #fix\n",
    "    tol = st.multiselect('Tolerance', [0.0001, 0.001, 0.05, 0.1], [0.0001])\n",
    "    penalty = st.multiselect('Penalty', ['l2', 'l1', 'elasticnet'], ['l2']) \n",
    "    alpha = st.multiselect('Alpha', [0.0001, 0.001, 0.05, 0.1, 0.2, 0.3], [0.0001])\n",
    "    n_jobs = st.selectbox('Parallelization', [1, 2, 3, 4, -1], 4)\n",
    "    \n",
    "    sgd_params['clf__estimator__loss'] = loss\n",
    "    sgd_params['clf__estimator__max_iter'] = max_iter\n",
    "    sgd_params['clf__estimator__tol'] = tol\n",
    "    sgd_params['clf__estimator__penalty'] = penalty\n",
    "    sgd_params['clf__estimator__alpha'] = alpha\n",
    "    sgd_params['clf__estimator__n_jobs'] = [n_jobs]\n",
    "    return sgd_params\n",
    "\n",
    "def NN():\n",
    "    nn_params = {'clf__estimator': [MLPClassifier()]\n",
    "                }\n",
    "    \n",
    "    st.subheader('Neural Network')\n",
    "    solver = st.multiselect('Solver', ['lbfgs', 'sgd', 'adam'], ['adam'])\n",
    "    max_iter = st.multiselect('Max Iterations', [1000,1100,1200,1300,1400], [1000])\n",
    "    alpha = st.multiselect('Alpha', list(10.0 ** -np.arange(1, 10)), [0.0001])\n",
    "    hidden_layer_sizes = st.multiselect('Hidden Layer Sizes', list(range(50, 500, 50)), [100])\n",
    "#     hidden_layer_sizes = st.multiselect('Hidden Layer Sizes', [50, 100, 150, 200, 250, 300, 350, 400, 450, 500] , [100])\n",
    "    \n",
    "    nn_params['clf__estimator__solver'] = solver\n",
    "    nn_params['clf__estimator__max_iter'] = max_iter\n",
    "    nn_params['clf__estimator__alpha'] = alpha\n",
    "    nn_params['clf__estimator__hidden_layer_sizes'] = hidden_layer_sizes\n",
    "    return nn_params\n",
    "\n",
    "\n",
    "data = st.file_uploader('Upload a csv')\n",
    "test_data = st.file_uploader('Upload a csv for prediction:') \n",
    "\n",
    "if (data != None) & (test_data != None):\n",
    "    df = pd.read_csv(data)\n",
    "    df_test = pd.read_csv(test_data)\n",
    "#     df = random.shuffle(data)\n",
    "    target_col =st.selectbox('Choose target variable', df.columns)\n",
    "    X = df.drop(target_col, axis = 1)\n",
    "    y = df[target_col]\n",
    "    test_ratio = st.number_input('Enter test split ratio, 0 < ratio < 1', min_value = 0.0, \n",
    "                                 max_value = 1.0, value = 0.2)\n",
    "    \n",
    "    if test_ratio:\n",
    "        X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y,\n",
    "                                                                        test_size=test_ratio,\n",
    "                                                                        random_state = 0)\n",
    "        \n",
    "        enc = LabelEncoder()\n",
    "        trans_y_train = enc.fit_transform(y_train)\n",
    "\n",
    "        selected_models = st.sidebar.multiselect(\n",
    "                              'Choose Algorithms:',(\n",
    "                                  'Gaussian NB',\n",
    "                                  'Logistic Regression', \n",
    "                                  'KNN',\n",
    "                                  'Support Vector Machines',\n",
    "                                  'Decision Tree',\n",
    "                                  'Random Forest',\n",
    "                                  'Gradient Boosting',\n",
    "                                  'Extra Random Trees',\n",
    "                                  'XGBoost',\n",
    "                                  'Stochastic Gradient Descent',\n",
    "                                  'Neural Network'), ['KNN', 'Support Vector Machines', 'Decision Tree'])\n",
    "        \n",
    "#         selected_models = st.sidebar.multiselect(\n",
    "#                           'Choose Algorithms:',(\n",
    "#                               'Gaussian NB',\n",
    "#                               'Logistic Regression',))\n",
    "\n",
    "\n",
    "    \n",
    "        if selected_models:\n",
    "#         gnb_params = {'clf__estimator': [GaussianNB()]\n",
    "#                      }\n",
    "#         lr_params = {'clf__estimator': [LogisticRegression()]\n",
    "#                     }\n",
    "\n",
    "#         st.subheader('Logistic Regression')\n",
    "#         penalty = st.multiselect('Penalty', ['l1', 'l2'])\n",
    "#         reg = st.multiselect('C', [0.1, 1.0, 2.0], [1.0])\n",
    "#         solver = st.multiselect('Solver', ['liblinear', 'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], ['liblinear'])\n",
    "        \n",
    "#         lr_params = LogisticReg()\n",
    "#         param_dict = {'Gaussian NB': gnb_params,\n",
    "#                           'Logistic Regression':lr_params, \n",
    "#          \n",
    "    \n",
    "            func_dict = {'Gaussian NB': GNB(),\n",
    "                         'Logistic Regression':LogisticReg(), \n",
    "                          'KNN': KNN(),\n",
    "                          'Support Vector Machines': SVM(),\n",
    "                          'Decision Tree': DT(),\n",
    "                          'Random Forest': RF(),\n",
    "                          'Gradient Boosting': GB(),\n",
    "                          'Extra Random Trees': ERT(),\n",
    "                          'XGBoost': XGB(),\n",
    "                          'Stochastic Gradient Descent': SGD(),\n",
    "                          'Neural Network': NN()\n",
    "                        }\n",
    "        \n",
    "#             func_dict  = {'Gaussian NB': GNB(),\n",
    "#                           'Logistic Regression':LogisticReg()\n",
    "#              }\n",
    "    \n",
    "            param_dict = {}\n",
    "\n",
    "            for i in selected_models:\n",
    "                param_dict[i] = func_dict[i]\n",
    "\n",
    "\n",
    "            cardinality = 10\n",
    "\n",
    "            # \"Cardinality\" means the number of unique values in a column\n",
    "            # Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "            categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < cardinality and \n",
    "                                    X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "            # # Select numerical columns\n",
    "            numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "            # # Keep selected columns only\n",
    "            my_cols = categorical_cols + numerical_cols\n",
    "            # X_train = pd.DataFrame(X_train_full[my_cols].copy(), columns = X.columns)\n",
    "            # X_valid = pd.DataFrame(X_valid_full[my_cols].copy(), columns = X.columns)\n",
    "            X_train = pd.DataFrame(X_train_full[my_cols].copy(), columns = my_cols)\n",
    "            X_valid = pd.DataFrame(X_valid_full[my_cols].copy(), columns = my_cols)\n",
    "            data_valid = pd.concat([X_valid, y_valid], axis = 1)\n",
    "\n",
    "            # # Preprocessing for numerical data\n",
    "            numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "            # Preprocessing for categorical data\n",
    "            categorical_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "            ])\n",
    "\n",
    "            # Bundle preprocessing for numerical and categorical data\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('num', numerical_transformer, numerical_cols),\n",
    "                    ('cat', categorical_transformer, categorical_cols)\n",
    "                ])\n",
    "            #poly = Pipeline(steps = [('poly',PolynomialFeatures(2))])\n",
    "            # poly = PolynomialFeatures(2)\n",
    "\n",
    "            #Principal Component Analysis to get remove noisy features\n",
    "            #pca = PCA(n_components = 100)\n",
    "\n",
    "            from sklearn.base import BaseEstimator\n",
    "\n",
    "            class MyClassifier(BaseEstimator):\n",
    "\n",
    "                def __init__(\n",
    "                    self, \n",
    "                    estimator = XGBClassifier(),\n",
    "                ):\n",
    "                    \"\"\"\n",
    "                    A Custom BaseEstimator that can switch between classifiers.\n",
    "                    :param estimator: sklearn object - The classifier\n",
    "                    \"\"\" \n",
    "\n",
    "                    self.estimator = estimator\n",
    "\n",
    "\n",
    "                def fit(self, X, y=None, **kwargs):\n",
    "                    self.estimator.fit(X, y)\n",
    "                    return self\n",
    "\n",
    "\n",
    "                def predict(self, X, y=None):\n",
    "                    return self.estimator.predict(X)\n",
    "\n",
    "\n",
    "                def predict_proba(self, X):\n",
    "                    return self.estimator.predict_proba(X)\n",
    "\n",
    "\n",
    "                def score(self, X, y):\n",
    "                    return self.estimator.score(X, y)\n",
    "                \n",
    "                @property\n",
    "                def classes_(self):\n",
    "                    return self.estimator.classes_\n",
    "\n",
    "            # Bundle preprocessing and modeling code in a pipeline\n",
    "            my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "\n",
    "                                          ('clf', MyClassifier())\n",
    "                                         ])\n",
    "\n",
    "            parameters = []\n",
    "\n",
    "            for i in selected_models:\n",
    "                parameters.append(param_dict[i])\n",
    "            st.write(parameters)\n",
    "            train = st.button('Train Model')\n",
    "            if train:\n",
    "                with st.spinner('Training Model...'):\n",
    "                    from sklearn.model_selection import GridSearchCV\n",
    "                    gscv = GridSearchCV(my_pipeline, parameters, cv=3, n_jobs=-1, return_train_score=False, verbose=3)\n",
    "                    gscv.fit(X_train, y_train)\n",
    "                    st.text('Best Parameters')\n",
    "                    st.write(gscv.best_params_)\n",
    "                    st.text('Best Score')\n",
    "                    st.write(gscv.best_score_)\n",
    "                    data_valid['Predicted'] = gscv.predict(X_valid)\n",
    "                    st.write(data_valid)\n",
    "                    cm = confusion_matrix(y_valid, gscv.predict(X_valid))  \n",
    "                    st.text('Confusion Matrix')\n",
    "                    fig1, ax1 = plt.subplots()\n",
    "                    class_names = y_valid.unique()\n",
    "                    confusion_matrix_plot(cm, class_names)\n",
    "                    st.pyplot(fig1)\n",
    "                    st.text('Performance Metrics')\n",
    "                    st.write(combined_metrics(X_valid, y_valid, gscv))\n",
    "                    df_test['Predicted'] = gscv.predict(df_test)\n",
    "                    st.write(df_test)\n",
    "#                     st.write(X_train)\n",
    "                    explainer = shap.KernelExplainer(model_predict, X_train_full)\n",
    "                    shap_values = explainer.shap_values(X_valid_full.iloc[2,:])\n",
    "                    st.pyplot(shap.force_plot(explainer.expected_value[0], shap_values[0], X_valid_full.iloc[2,:], matplotlib=True, text_rotation=8))\n",
    "                    \n",
    "                    f = lambda x: model_predict(x)[:,1]\n",
    "                    med = X_train_full.median().values.reshape((1,X_train_full.shape[1]))\n",
    "\n",
    "                    explainer = shap.Explainer(f, med)\n",
    "                    shap_values = explainer(X_valid_full.iloc[0:1000,:])\n",
    "                    st.pyplot(shap.plots.waterfall(shap_values[10]))\n",
    "\n",
    "                    pdp_ = pdp.pdp_isolate(model=gscv, dataset=X_train_full,\n",
    "                                                model_features=X_train_full.columns,\n",
    "                                                feature=my_cols[0])\n",
    "#                     st.write(pdp_)\n",
    "                    fig, axes = pdp.pdp_plot(pdp_isolate_out=pdp_, feature_name=my_cols[0], center = True, ncols=1, figsize = (15, 10))\n",
    "                    st.pyplot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_Classification.ipynb.\n",
      "Converted 01_Regression.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
